{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gCQagsTmP2Y"
   },
   "source": [
    "# Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qlq6CvnuS6w"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for API querying\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Imports for image scaling and file access\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For splitting into train and test\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# For PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be loading API keys and related secret value we will be using .env files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function which will be useful later on in nutrition data retrieval will be to get a list of all the food names present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W4JKIDQwPZq",
    "outputId": "51759af3-4dd2-4d1c-bc6e-a43896271864"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_food_names():\n",
    "    data_path = 'food-128'\n",
    "    food_names = []\n",
    "\n",
    "    # Extract food names from folders\n",
    "    for food_name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, food_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            food_names.append(food_name.replace('_', ' '))\n",
    "    return food_names\n",
    "print(get_food_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the images will help massively in getting a working model without needing to use the full image data. The method to do so is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6opMW_FUzvtJ"
   },
   "outputs": [],
   "source": [
    "# Downscale images (ChatGPT)\n",
    "# Take our raw images\n",
    "\n",
    "# Source and target directories\n",
    "SRC_DIR = \"images\"        # original images folder with class subfolders\n",
    "DST_DIR = \"food-128\"      # output folder\n",
    "\n",
    "size = (128, 128)  # target resolution\n",
    "\n",
    "# Create output root directory\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through every class folder\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    src_class_path = os.path.join(SRC_DIR, class_name)\n",
    "    dst_class_path = os.path.join(DST_DIR, class_name)\n",
    "\n",
    "    # Skip non-directories (just in case)\n",
    "    if not os.path.isdir(src_class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(dst_class_path, exist_ok=True)\n",
    "\n",
    "    # Resize each image\n",
    "    for img_name in tqdm(os.listdir(src_class_path), desc=f\"Resizing {class_name[:20]}\"):\n",
    "        src_img_path = os.path.join(src_class_path, img_name)\n",
    "        dst_img_path = os.path.join(dst_class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_img_path).convert(\"RGB\")\n",
    "            img = img.resize(size, Image.Resampling.BILINEAR)\n",
    "            img.save(dst_img_path, quality=90)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {src_img_path}: {e}\")\n",
    "\n",
    "print(\"All images resized to 128x128 and saved in food-128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoodData Central API Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading the API data, we'll first need to define some constants which will be used through data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup\n",
    "FDC_BASE_URL = 'https://api.nal.usda.gov/fdc/v1'\n",
    "FDC_API_KEY = os.getenv('FDC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search the FoodData Central database, we'll use the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_food(query, data_type='Survey (FNDDS)', limit=10):\n",
    "    # data_type can be any of 'Branded', 'Foundation', 'Survey (FNDDS)', 'SR Legacy'\n",
    "    parameters = {\n",
    "        'api_key': FDC_API_KEY,\n",
    "        'query': query,\n",
    "        'dataType': data_type,\n",
    "        'pageSize': limit\n",
    "    }\n",
    "    response = requests.get(url=f'{FDC_BASE_URL}/foods/search', params=parameters)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_calories(food):\n",
    "    for nutrient in food.get('foodNutrients', []):\n",
    "        # TODO: test if {foodNutrientId': 34350077, nutrientName: 'Energy'} is the correct dict\n",
    "        if nutrient.get('nutrientName') == 'Energy' and nutrient.get('unitName') == 'KCAL':\n",
    "            return nutrient['value']\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_results(data):\n",
    "    foods = data.get('foods', [])\n",
    "    result_foods = []\n",
    "    for food in foods:\n",
    "        calories = get_calories(food)\n",
    "        if calories is not None:\n",
    "            result_foods.append({\n",
    "                'name': food['description'],\n",
    "                'calories': calories\n",
    "            })\n",
    "    return result_foods\n",
    "\n",
    "\n",
    "def estimate_calories_from_results(results, top_n=5):\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Get the first `top_n` calorie values that aren't None\n",
    "    calories = [r['calories'] for r in results if r.get('calories') is not None][:top_n]\n",
    "    if not calories:\n",
    "        return None\n",
    "    \n",
    "    return round(sum(calories) / len(calories), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the helper functions' ability to query the API, we can search with a given dish name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the API calls could fail, use a try except block\n",
    "try:\n",
    "    food = input('Type a food:')\n",
    "    response = search_food(food)\n",
    "    simplified = simplify_results(response)\n",
    "    print(len(simplified))\n",
    "    pprint(simplified)\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to refine this data to contain only the relevant caloric information for the foods in our dataset. An important aspect of this to note is that the FoodData Central API is limited to 1000 requests per hour by IP address, and will block an IP address for one hour if this limit is exceded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_names = get_food_names()\n",
    "    all_calorie_data = []\n",
    "    for food in food_names:\n",
    "        response = search_food(food)\n",
    "        simplified = simplify_results(response)\n",
    "        estimate = estimate_calories_from_results(simplified, top_n=5)\n",
    "        print(f\"\\n{food}: ~{estimate} kcal/100g\")\n",
    "        all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': simplified\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous code block, we now have a list of the results of the keyword search for all 101 dishes along with an estimate for kcal/100g. However, some of these contained no entries at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and record missing dishes\n",
    "missing_dishes = []\n",
    "for i in range(len(food_names)):\n",
    "    if all_calorie_data[i].get('estimate_calories_per_100g') == None:\n",
    "        missing_dishes.append(food_names[i])\n",
    "\n",
    "print('Missing dish count:', len(missing_dishes))\n",
    "pprint(missing_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To supplement the data which doesn't have an entry in the FoodData Central API, we'll be using the Nutritionix API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRITIONIX_URL = 'https://trackapi.nutritionix.com/v2/natural/nutrients'\n",
    "NUTRITIONIX_APP_ID = os.getenv('NUTRITIONIX_APP_ID')\n",
    "NUTRITIONIX_APP_KEY = os.getenv('NUTRITIONIX_APP_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calories_nutritionix(query):\n",
    "    headers = {\n",
    "        'x-app-id': NUTRITIONIX_APP_ID,\n",
    "        'x-app-key': NUTRITIONIX_APP_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    body = {\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(NUTRITIONIX_URL, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    foods = data.get(\"foods\", [])\n",
    "    if not foods:\n",
    "        print(\"No nutrition data found for\", query)\n",
    "        return None\n",
    "\n",
    "    # Extract calorie info from first result\n",
    "    f = foods[0]\n",
    "    cal = f.get(\"nf_calories\", 0)\n",
    "    weight = f.get(\"serving_weight_grams\", 100)\n",
    "    cal_per_100g = cal * (100 / weight)\n",
    "\n",
    "    return round(cal_per_100g, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When storing this with the rest of our data which had FoodData Central simplified results included, we'll use `None` as the value for the `'results'` kay in the food dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food in missing_dishes:\n",
    "    estimate = get_calories_nutritionix(food)\n",
    "    all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': None\n",
    "        })\n",
    "    print(f'{food}: ', estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food_entry in all_calorie_data:\n",
    "    pprint(food_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn Image Data Into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = 'food-128'\n",
    "\n",
    "TRAIN_DIR = 'food-128-split/train'\n",
    "TEST_DIR = 'food-128-split/test'\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(TRAIN_DIR)\n",
    "os.makedirs(TEST_DIR)\n",
    "\n",
    "cntr = 0\n",
    "# Loop through each food subclass\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    class_path = os.path.join(SRC_DIR, class_name)\n",
    "\n",
    "    # Ensure its a folder\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # Shuffle images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Set splitting point\n",
    "        split_idx = int(len(images) * 0.8)\n",
    "        \n",
    "        train_images = images[:split_idx]\n",
    "        test_images = images[split_idx:]\n",
    "\n",
    "        # Create new food folders in train and test\n",
    "        train_class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        test_class_dir = os.path.join(TEST_DIR, class_name)\n",
    "        os.makedirs(train_class_dir)\n",
    "        os.makedirs(test_class_dir)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        cntr += 1\n",
    "        print(f\"{cntr}. Processesed {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "print(\"Dataset split successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Images to new format using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        # These numbers come from ImageNet\n",
    "        # Represent mean and standard deviation values for RGB based on a massive dateset of color distribution in natural images\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load training and test sets\n",
    "train_dataset = datasets.ImageFolder(root = 'food-128-split/train', transform = transform)\n",
    "test_dataset = datasets.ImageFolder(root = 'food-128-split/test', transform = transform)\n",
    "\n",
    "# Create PyTorch data loaders for later\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = 32, # use 32 images per batch\n",
    "    shuffle = True, # shuffle training data each time to not get false patterns\n",
    "    num_workers = 4 # use 4 CPU threads in parallel to speed up work\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 32, # same as above\n",
    "    shuffle = False, # don't shuffle test data for consistent benchmarks\n",
    "    num_workers = 4 # same as above\n",
    ")\n",
    "\n",
    "# Verify stuff\n",
    "class_cnt = len(train_dataset.classes)\n",
    "# Get class count\n",
    "print(f\"Num Classes: {class_cnt}\")\n",
    "# See a few just to verify names saved correctly\n",
    "print(f\"Sample Classes: {train_dataset.classes[:10]}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "calorie-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
