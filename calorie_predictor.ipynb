{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gCQagsTmP2Y"
   },
   "source": [
    "# 1. Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qlq6CvnuS6w"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for API querying\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Imports for image scaling and file access\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For splitting into train and test\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# For EDA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# For calculations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be loading API keys and related secret value we will be using .env files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function which will be useful later on in nutrition data retrieval will be to get a list of all the food names present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W4JKIDQwPZq",
    "outputId": "51759af3-4dd2-4d1c-bc6e-a43896271864"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_food_names():\n",
    "    data_path = 'food-128'\n",
    "    food_names = []\n",
    "\n",
    "    # Extract food names from folders\n",
    "    for food_name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, food_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            food_names.append(food_name.replace('_', ' '))\n",
    "    return food_names\n",
    "print(get_food_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the images will help massively in getting a working model without needing to use the full image data. The method to do so is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6opMW_FUzvtJ"
   },
   "outputs": [],
   "source": [
    "# Downscale images (ChatGPT)\n",
    "# Take our raw images\n",
    "\n",
    "# Source and target directories\n",
    "SRC_DIR = \"images\"        # original images folder with class subfolders\n",
    "DST_DIR = \"food-128\"      # output folder\n",
    "\n",
    "size = (128, 128)  # target resolution\n",
    "\n",
    "# Create output root directory\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through every class folder\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    src_class_path = os.path.join(SRC_DIR, class_name)\n",
    "    dst_class_path = os.path.join(DST_DIR, class_name)\n",
    "\n",
    "    # Skip non-directories (just in case)\n",
    "    if not os.path.isdir(src_class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(dst_class_path, exist_ok=True)\n",
    "\n",
    "    # Resize each image\n",
    "    for img_name in tqdm(os.listdir(src_class_path), desc=f\"Resizing {class_name[:20]}\"):\n",
    "        src_img_path = os.path.join(src_class_path, img_name)\n",
    "        dst_img_path = os.path.join(dst_class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_img_path).convert(\"RGB\")\n",
    "            img = img.resize(size, Image.Resampling.BILINEAR)\n",
    "            img.save(dst_img_path, quality=90)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {src_img_path}: {e}\")\n",
    "\n",
    "print(\"All images resized to 128x128 and saved in food-128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodData Central API Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading the API data, we'll first need to define some constants which will be used through data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup\n",
    "FDC_BASE_URL = 'https://api.nal.usda.gov/fdc/v1'\n",
    "FDC_API_KEY = os.getenv('FDC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search the FoodData Central database, we'll use the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_food(query, data_type='Survey (FNDDS)', limit=10):\n",
    "    # data_type can be any of 'Branded', 'Foundation', 'Survey (FNDDS)', 'SR Legacy'\n",
    "    parameters = {\n",
    "        'api_key': FDC_API_KEY,\n",
    "        'query': query,\n",
    "        'dataType': data_type,\n",
    "        'pageSize': limit\n",
    "    }\n",
    "    response = requests.get(url=f'{FDC_BASE_URL}/foods/search', params=parameters)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_calories(food):\n",
    "    for nutrient in food.get('foodNutrients', []):\n",
    "        # TODO: test if {foodNutrientId': 34350077, nutrientName: 'Energy'} is the correct dict\n",
    "        if nutrient.get('nutrientName') == 'Energy' and nutrient.get('unitName') == 'KCAL':\n",
    "            return nutrient['value']\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_results(data):\n",
    "    foods = data.get('foods', [])\n",
    "    result_foods = []\n",
    "    for food in foods:\n",
    "        calories = get_calories(food)\n",
    "        if calories is not None:\n",
    "            result_foods.append({\n",
    "                'name': food['description'],\n",
    "                'calories': calories\n",
    "            })\n",
    "    return result_foods\n",
    "\n",
    "\n",
    "def estimate_calories_from_results(results, top_n=5):\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Get the first `top_n` calorie values that aren't None\n",
    "    calories = [r['calories'] for r in results if r.get('calories') is not None][:top_n]\n",
    "    if not calories:\n",
    "        return None\n",
    "    \n",
    "    return round(sum(calories) / len(calories), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the helper functions' ability to query the API, we can search with a given dish name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the API calls could fail, use a try except block\n",
    "try:\n",
    "    food = input('Type a food:')\n",
    "    response = search_food(food)\n",
    "    simplified = simplify_results(response)\n",
    "    print(len(simplified))\n",
    "    pprint(simplified)\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to refine this data to contain only the relevant caloric information for the foods in our dataset. An important aspect of this to note is that the FoodData Central API is limited to 1000 requests per hour by IP address, and will block an IP address for one hour if this limit is exceded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_names = get_food_names()\n",
    "    all_calorie_data = []\n",
    "    for food in food_names:\n",
    "        response = search_food(food)\n",
    "        simplified = simplify_results(response)\n",
    "        estimate = estimate_calories_from_results(simplified, top_n=5)\n",
    "        print(f\"\\n{food}: ~{estimate} kcal/100g\")\n",
    "        all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': simplified\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous code block, we now have a list of the results of the keyword search for all 101 dishes along with an estimate for kcal/100g. However, some of these contained no entries at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and record missing dishes\n",
    "missing_dishes = []\n",
    "for i in range(len(food_names)):\n",
    "    if all_calorie_data[i].get('estimate_calories_per_100g') == None:\n",
    "        missing_dishes.append(food_names[i])\n",
    "\n",
    "print('Missing dish count:', len(missing_dishes))\n",
    "pprint(missing_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To supplement the data which doesn't have an entry in the FoodData Central API, we'll be using the Nutritionix API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRITIONIX_URL = 'https://trackapi.nutritionix.com/v2/natural/nutrients'\n",
    "NUTRITIONIX_APP_ID = os.getenv('NUTRITIONIX_APP_ID')\n",
    "NUTRITIONIX_APP_KEY = os.getenv('NUTRITIONIX_APP_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calories_nutritionix(query):\n",
    "    headers = {\n",
    "        'x-app-id': NUTRITIONIX_APP_ID,\n",
    "        'x-app-key': NUTRITIONIX_APP_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    body = {\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(NUTRITIONIX_URL, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    foods = data.get(\"foods\", [])\n",
    "    if not foods:\n",
    "        print(\"No nutrition data found for\", query)\n",
    "        return None\n",
    "\n",
    "    # Extract calorie info from first result\n",
    "    f = foods[0]\n",
    "    cal = f.get(\"nf_calories\", 0)\n",
    "    weight = f.get(\"serving_weight_grams\", 100)\n",
    "    cal_per_100g = cal * (100 / weight)\n",
    "\n",
    "    return round(cal_per_100g, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When storing this with the rest of our data which had FoodData Central simplified results included, we'll use `None` as the value for the `'results'` key in the food dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food in missing_dishes:\n",
    "    estimate = get_calories_nutritionix(food)\n",
    "    all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': None\n",
    "        })\n",
    "    print(f'{food}: ', estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food_entry in all_calorie_data:\n",
    "    pprint(food_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier use later on, we'll save the calorie estimate information to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe then store to CSV\n",
    "rows = [\n",
    "    {'food': entry['food'], 'kcal/100g': entry['estimate_calories_per_100g']}\n",
    "    for entry in all_calorie_data\n",
    "]\n",
    "\n",
    "calorie_df = pd.DataFrame(rows, columns=['food', 'kcal/100g'])\n",
    "print(calorie_df.info())\n",
    "\n",
    "calorie_df.to_csv('data/calories.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn Image Data Into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = 'food-128'\n",
    "\n",
    "TRAIN_DIR = 'food-128-split/train'\n",
    "TEST_DIR = 'food-128-split/test'\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(TRAIN_DIR)\n",
    "os.makedirs(TEST_DIR)\n",
    "\n",
    "cntr = 0\n",
    "# Loop through each food subclass\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    class_path = os.path.join(SRC_DIR, class_name)\n",
    "\n",
    "    # Ensure its a folder\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # Shuffle images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Set splitting point\n",
    "        split_idx = int(len(images) * 0.8)\n",
    "        \n",
    "        train_images = images[:split_idx]\n",
    "        test_images = images[split_idx:]\n",
    "\n",
    "        # Create new food folders in train and test\n",
    "        train_class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        test_class_dir = os.path.join(TEST_DIR, class_name)\n",
    "        os.makedirs(train_class_dir)\n",
    "        os.makedirs(test_class_dir)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        cntr += 1\n",
    "        print(f\"{cntr}. Processesed {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "print(\"Dataset split successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Images to new format using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        # These numbers come from ImageNet\n",
    "        # Represent mean and standard deviation values for RGB based on a massive dateset of color distribution in natural images\n",
    "        mean = [0.485, 0.456, 0.406],   \n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load training and test sets\n",
    "train_dataset = datasets.ImageFolder(root = 'food-128-split/train', transform = transform)\n",
    "test_dataset = datasets.ImageFolder(root = 'food-128-split/test', transform = transform)\n",
    "\n",
    "# Create PyTorch data loaders for later\n",
    "train_data = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = 32, # use 32 images per batch\n",
    "    shuffle = True, # shuffle training data each time to not get false patterns\n",
    "    num_workers = 4 # use 4 CPU threads in parallel to speed up work\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 32, # same as above\n",
    "    shuffle = False, # don't shuffle test data for consistent benchmarks\n",
    "    num_workers = 4 # same as above\n",
    ")\n",
    "\n",
    "# Verify stuff\n",
    "class_cnt = len(train_dataset.classes)\n",
    "# Get class count\n",
    "print(f\"Num Classes: {class_cnt}\")\n",
    "# See a few just to verify names saved correctly\n",
    "print(f\"Sample Classes: {train_dataset.classes[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Calorie Estimate EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calorie data loading and summary statistics\n",
    "CALORIE_FILE_PATH = 'data/calories.csv'\n",
    "calorie_df = pd.read_csv(CALORIE_FILE_PATH)\n",
    "\n",
    "print(calorie_df.info())\n",
    "print(calorie_df.describe())\n",
    "print(calorie_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Calorie Visualization\n",
    "# Begin by sorting df by 'kcal/100g'\n",
    "calorie_df.sort_values('kcal/100g', ascending=False, inplace=True)\n",
    "calorie_df.reindex()\n",
    "\n",
    "# Create bar chart of the kcal/100g estimates for each food\n",
    "plt.figure(figsize=(5, 18))\n",
    "plt.title('Bar Chart of Calorie Estimates')\n",
    "plt.barh(calorie_df['food'], calorie_df['kcal/100g'])\n",
    "plt.ylabel('Food Name')\n",
    "plt.xlabel('Estimated kcal/100g')\n",
    "plt.margins(y = 0.005)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data EDA\n",
    "Generate visuals for the average pixel values of each food class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Images to tensor (fancy 4D array)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset using this transform\n",
    "dataset = datasets.ImageFolder(root='food-128-split/train', transform = transform)\n",
    "\n",
    "# Get names of all foods\n",
    "food_names = dataset.classes\n",
    "total_num_foods = len(food_names) # should always be 101 but just in case\n",
    "\n",
    "# 7 distinct food to visualize\n",
    "selected_classes = [\n",
    "    'sushi',\n",
    "    'guacamole',\n",
    "    'spaghetti_bolognese',\n",
    "    'macarons',\n",
    "    'chocolate_cake',\n",
    "    'caesar_salad',\n",
    "    'strawberry_shortcake',\n",
    "    'beef_carpaccio'\n",
    "]\n",
    "food_count = len(selected_classes)\n",
    "\n",
    "# Map class names to integer labels used by ImageFolder\n",
    "selected_indices = [dataset.class_to_idx[c] for c in selected_classes]\n",
    "\n",
    "# Filter dataset to only images with those labels\n",
    "subset_indices = [\n",
    "    i for i, (_, label) in enumerate(dataset.samples)\n",
    "    if label in selected_indices\n",
    "]\n",
    "subset_dataset = Subset(dataset, subset_indices) # create subset of just selected indices\n",
    "\n",
    "# Create data loader because its much faster\n",
    "subset_loader = DataLoader(\n",
    "    subset_dataset,\n",
    "    batch_size=32,   # process 32 images at once\n",
    "    shuffle=False,   # don't randomize order for this\n",
    "    num_workers=4    # load using 4 CPU threads in parallel\n",
    ")\n",
    "\n",
    "# Get tensors ready to find averages\n",
    "H, W = 128, 128\n",
    "# Track sum of R,G,B values for each pixel position\n",
    "# Torch.zeros is 4D array we can use to keep values for each pixel distinct\n",
    "sums = torch.zeros((food_count, 3, H, W))\n",
    "# Keep track of how many images you've seen in current food class (again should always be 101 for our case)\n",
    "counts = torch.zeros(food_count)\n",
    "# Keep track of running sum of squares\n",
    "sum_sq = torch.zeros((food_count, 3, H, W))\n",
    "\n",
    "# Map original labels (0-100) to new compact label (0..food_count-1)\n",
    "class_map = {orig_label: i for i, orig_label in enumerate(selected_indices)}\n",
    "\n",
    "# Load through batches and add up pixel values\n",
    "for images, labels in subset_loader: # images is tensor of batch size, 3, H, W --- lables is tensor of batch size with label for each image (will be the same label except when transitioning between food types)\n",
    "    for i in range(images.size(0)):  # iterate through each image in batch\n",
    "        label = labels[i].item() # get current image label\n",
    "        if label in class_map:\n",
    "            idx = class_map[label] # get idx in new smaller array\n",
    "            sums[idx] += images[i] # add current image pixel values\n",
    "            counts[idx] += 1 # track that we've added another image worth of pixels for later division\n",
    "            sum_sq[idx] += images[i] ** 2\n",
    "\n",
    "# Calculate average image (pixel-wise division)\n",
    "# view reshapes counts into food_count, 1, 1, 1 so it works correctly when dividing into 4D sums tensor\n",
    "avg_images = (sums / counts.view(-1, 1, 1, 1))\n",
    "\n",
    "# Pixel-wise standard deviation\n",
    "std_images = torch.sqrt((sum_sq / counts.view(-1, 1, 1, 1)) - (avg_images ** 2))\n",
    "\n",
    "# Get std between different classes\n",
    "interclass_std = torch.std(avg_images, dim=0)\n",
    "\n",
    "# Calculate average pixel for each average image\n",
    "avg_pixels = avg_images.mean(dim=(2, 3)) # avg_pixels is a tensor of batch size, 3, where each of the values are the means from dims H and W or avg_images\n",
    "\n",
    "# Visualize average images\n",
    "fig, axes = plt.subplots(math.ceil(food_count / 2), 2, figsize=(5, math.ceil(food_count / 2) * 2.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, orig_label in enumerate(selected_indices):\n",
    "    avg_img = avg_images[i].permute(1, 2, 0).numpy()  # Pytorch keeps images in Color, Height, Width, Order but matplotlib uses Height, Width, Color\n",
    "    axes[i].imshow(avg_img)\n",
    "    axes[i].set_title(food_names[orig_label], fontsize=12)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize average pixel channel values\n",
    "fig, axes = plt.subplots(math.ceil(food_count / 2), 2, figsize=(6, math.ceil(food_count / 2) * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, orig_label in enumerate(selected_indices):\n",
    "    channel_names = ['R', 'G', 'B']\n",
    "    values = [avg_pixels[i, 0], avg_pixels[i, 1], avg_pixels[i, 2]]\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    axes[i].bar(channel_names, values, color=colors)\n",
    "    axes[i].set_title(food_names[orig_label])\n",
    "    axes[i].set_xlabel('Color Channel')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].set_xmargin(0.1)\n",
    "    axes[i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute overall std value for each class\n",
    "class_std_values = std_images.mean(dim=(1, 2, 3))\n",
    "\n",
    "# Plot std values for deviation in average color between classes\n",
    "# Possible future improvement could be using a circlular subset of the image to get the average color to avoid background noise\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh([food_names[i] for i in selected_indices], class_std_values.numpy())\n",
    "plt.xlabel(\"Average Standard Deviation\")\n",
    "plt.title(\"Overall Color Variability per Food Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert to numpy image and reorder to HWC for matplotlib\n",
    "interclass_std_img = interclass_std.permute(1, 2, 0).numpy()\n",
    "# Normalize for better contrast (to highlight areas with most deviation)\n",
    "interclass_std_img = interclass_std_img / interclass_std_img.max()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(interclass_std_img)\n",
    "plt.title(\"Standard Deviation Across Classes\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Numerical Summaries for Entire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels and image dimensions (assume all images have same shape)\n",
    "sample_img, _ = dataset[0]\n",
    "num_channels, H, W = sample_img.shape\n",
    "\n",
    "# Count images per class\n",
    "labels = [label for _, label in dataset]\n",
    "counts_per_class = Counter(labels)\n",
    "\n",
    "# Map label index to class name\n",
    "class_names = dataset.classes\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Image shape: {num_channels} channels, {H}x{W} pixels\")\n",
    "print(\"Number of images per class:\")\n",
    "for label, count in counts_per_class.items():\n",
    "    print(f\"  {class_names[label]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "Since we're working with 2D image data, using a CNN for classification is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define a basic CNN using PyTorch's Sequential model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN architecture\n",
    "\n",
    "# Layer filter counts\n",
    "num_conv1 = 32\n",
    "num_conv2 = 64\n",
    "num_conv3 = 128\n",
    "num_conv4 = 256\n",
    "num_hidden5 = 512\n",
    "num_output = 101\n",
    "\n",
    "num_convs = 4\n",
    "image_shape = 128\n",
    "k_size = 3          # kernel size\n",
    "p_size = 2          # pool size\n",
    "padding = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Block 1\n",
    "    #   Using in_channels=3 because of the three color channels. Each filter has each dimension halved.\n",
    "    nn.Conv2d(in_channels=3, out_channels=num_conv1, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 2\n",
    "    nn.Conv2d(in_channels=num_conv1, out_channels=num_conv2, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 3\n",
    "    nn.Conv2d(in_channels=num_conv2, out_channels=num_conv3, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 4\n",
    "    nn.Conv2d(in_channels=num_conv3, out_channels=num_conv4, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Classification\n",
    "    #   Hidden layer takes num_conv4 filters each of dimensions (image_shape // 2 ** num_convs, image_shape // 2 ** num_convs).\n",
    "    #   This layer produces the final output.\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=num_conv4 * (image_shape // 2 ** num_convs) ** 2, out_features=num_hidden5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_hidden5, out_features=num_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the CNN\n",
    "\n",
    "# Move to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Define criterion and optimizer to use in training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_data):\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# CNN Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Data Loading\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import split_dataset\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Data Loading Pipeline\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "seed = 42\n",
    "\n",
    "# 70% train, 30% temp split\n",
    "train_data = image_dataset_from_directory(\n",
    "    'food-128',\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "temp_data = image_dataset_from_directory(\n",
    "    'food-128',\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "# Split temp_data into 15% val, 15% test\n",
    "temp_batches = len(temp_data)\n",
    "val_batches = temp_batches // 2\n",
    "\n",
    "val_data = temp_data.take(val_batches)\n",
    "test_data = temp_data.skip(val_batches)\n",
    "\n",
    "class_names = train_data.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN architecture in tensorflow\n",
    "\n",
    "# Layer filter counts\n",
    "num_conv1 = 32\n",
    "num_conv2 = 64\n",
    "num_conv3 = 128\n",
    "num_conv4 = 256\n",
    "num_hidden5 = 512\n",
    "num_output = 101\n",
    "\n",
    "num_convs = 4\n",
    "image_shape = 128\n",
    "k_size = 3          # kernel size\n",
    "p_size = 2          # pool size\n",
    "padding = 1\n",
    "\n",
    "model = Sequential([\n",
    "    # Block 1\n",
    "    Conv2D(num_conv1, kernel_size=(k_size, k_size), activation='relu', input_shape=(image_shape, image_shape, 3)),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "    \n",
    "    # Block 2\n",
    "    Conv2D(num_conv2, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Block 3\n",
    "    Conv2D(num_conv3, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Block 4\n",
    "    Conv2D(num_conv4, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Classification Head\n",
    "    #   Hidden layer takes num_conv4 filters each of dimensions (image_shape // 2 ** num_convs, image_shape // 2 ** num_convs).\n",
    "    #   This layer produces the final output.\n",
    "    Flatten(),\n",
    "    Dense(num_hidden5, activation='relu'),\n",
    "    Dense(num_output, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "num_epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setup dataset\n",
    "image_size = 128\n",
    "batch_size = 32\n",
    "num_classes = 101\n",
    "\n",
    "# Define tensor flow\n",
    "train_ds = x\n",
    "val_ds = y\n",
    "test_ds = z\n",
    "\n",
    "# Optimize performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(AUTOTUNE).cache()\n",
    "val_ds = val_ds.prefetch(AUTOTUNE).cache()\n",
    "\n",
    "# Load pretrained ViT feature extractor from TF Hub\n",
    "hub_url = \"https://tfhub.dev/sayakpaul/vision_transformer_b32_fe/1\"\n",
    "feature_extractor_layer = hub.KerasLayer(hub_url, trainable=False,\n",
    "                                        input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Build the model\n",
    "inputs = keras.Input(shape=(image_size, image_size, 3))\n",
    "x = inputs\n",
    "x = feature_extractor_layer(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train\n",
    "epochs = 10\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs)\n",
    "\n",
    "# Optional Extras Below\n",
    "\"\"\" \n",
    "# (6) Optional: Unfreeze part of the feature extractor and fine-tune\n",
    "feature_extractor_layer.trainable = True\n",
    "# Maybe set a smaller learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "fine_tune_epochs = 5\n",
    "history_fine = model.fit(train_ds,\n",
    "                         validation_data=val_ds,\n",
    "                         epochs=epochs + fine_tune_epochs,\n",
    "                         initial_epoch=history.epoch[-1]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
