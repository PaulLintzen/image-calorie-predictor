{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gCQagsTmP2Y"
   },
   "source": [
    "# Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qlq6CvnuS6w"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for API querying\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Imports for image scaling and file access\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function which will be useful later on in nutrition data retrieval will be to get a list of all the food names present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W4JKIDQwPZq",
    "outputId": "51759af3-4dd2-4d1c-bc6e-a43896271864"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_food_names():\n",
    "    data_path = 'food-128'\n",
    "    food_names = []\n",
    "\n",
    "    # Extract food names from folders\n",
    "    for food_name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, food_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            food_names.append(food_name.replace('_', ' '))\n",
    "    return food_names\n",
    "print(get_food_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the images will help massively in getting a working model without needing to use the full image data. The method to do so is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6opMW_FUzvtJ"
   },
   "outputs": [],
   "source": [
    "# Downscale images (ChatGPT)\n",
    "# Take our raw images\n",
    "\n",
    "# Source and target directories\n",
    "SRC_DIR = \"images\"        # original images folder with class subfolders\n",
    "DST_DIR = \"food-128\"      # output folder\n",
    "\n",
    "size = (128, 128)  # target resolution\n",
    "\n",
    "# Create output root directory\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through every class folder\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    src_class_path = os.path.join(SRC_DIR, class_name)\n",
    "    dst_class_path = os.path.join(DST_DIR, class_name)\n",
    "\n",
    "    # Skip non-directories (just in case)\n",
    "    if not os.path.isdir(src_class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(dst_class_path, exist_ok=True)\n",
    "\n",
    "    # Resize each image\n",
    "    for img_name in tqdm(os.listdir(src_class_path), desc=f\"Resizing {class_name[:20]}\"):\n",
    "        src_img_path = os.path.join(src_class_path, img_name)\n",
    "        dst_img_path = os.path.join(dst_class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_img_path).convert(\"RGB\")\n",
    "            img = img.resize(size, Image.Resampling.BILINEAR)\n",
    "            img.save(dst_img_path, quality=90)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {src_img_path}: {e}\")\n",
    "\n",
    "print(\"All images resized to 128x128 and saved in food-128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoodData Central API Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading the API data, we'll first need to define some constants which will be used through data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup\n",
    "API_BASE_URL = 'https://api.nal.usda.gov/fdc/v1'\n",
    "API_KEY = 'Ujn2RHBzVpsD6gjTUl83caAgyjbUhEN2HjJcFmn9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search the FoodData Central database, we'll use the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_food(query, data_type='Survey (FNDDS)', limit=10):\n",
    "    # data_type can be any of 'Branded', 'Foundation', 'Survey (FNDDS)', 'SR Legacy'\n",
    "    parameters = {\n",
    "        'api_key': API_KEY,\n",
    "        'query': query,\n",
    "        'dataType': data_type,\n",
    "        'pageSize': limit\n",
    "    }\n",
    "    response = requests.get(url=f'{API_BASE_URL}/foods/search', params=parameters)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_calories(food):\n",
    "    for nutrient in food.get('foodNutrients', []):\n",
    "        # TODO: test if {foodNutrientId': 34350077, nutrientName: 'Energy'} is the correct dict\n",
    "        if nutrient.get('nutrientName') == 'Energy' and nutrient.get('unitName') == 'KCAL':\n",
    "            return nutrient['value']\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_results(data):\n",
    "    foods = data.get('foods', [])\n",
    "    result_foods = []\n",
    "    for food in foods:\n",
    "        calories = get_calories(food)\n",
    "        if calories is not None:\n",
    "            result_foods.append({\n",
    "                'name': food['description'],\n",
    "                'calories': calories\n",
    "            })\n",
    "    return result_foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the helper functions' ability to query the API, we can search with a given dish name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the API calls could fail, use a try except block\n",
    "try:\n",
    "    food = input('Type a food:')\n",
    "    response = search_food(food)\n",
    "    simplified = simplify_results(response)\n",
    "    print(len(simplified))\n",
    "    pprint(simplified)\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to refine this data to contain only the relevant caloric information for the foods in our dataset. An important aspect of this to note is that the FoodData Central API is limited to 1000 requests per hour by IP address, and will block an IP address for one hour if this limit is exceded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_names = get_food_names()\n",
    "    all_calorie_data = []\n",
    "    for food in food_names:\n",
    "        response = search_food(food)\n",
    "        simplified = simplify_results(response)\n",
    "        print('', food, sep='\\n')\n",
    "        pprint(simplified)\n",
    "        all_calorie_data.append(simplified)\n",
    "except Exception as e:\n",
    "    print('Error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous code block, we now have a list of the results of the keyword search for all 101 dishes. However, some of these contain no entries at all, and others contain many (limited to 10 for now) that may all be viable. In general, the first result seems to be the msot relevant, but there are exceptions. For now, we should find the names of food items which have no entry at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and record missing dishes and dishes with only 1 result\n",
    "missing_dishes = []\n",
    "single_dishes = []\n",
    "for i in range(len(food_names)):\n",
    "    if all_calorie_data[i] == []:\n",
    "        missing_dishes.append(food_names[i])\n",
    "    elif len(all_calorie_data[i]) == 1:\n",
    "        single_dishes.append(food_names[i])\n",
    "\n",
    "print('Missing dish count:', len(missing_dishes))\n",
    "pprint(missing_dishes)\n",
    "print('\\nSingle result dish count:', len(single_dishes))\n",
    "pprint(single_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn Image Data Into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "SRC_DIR = 'food-128'\n",
    "\n",
    "TRAIN_DIR = 'food-128-split/train'\n",
    "TEST_DIR = 'food-128-split/test'\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(TRAIN_DIR)\n",
    "os.makedirs(TEST_DIR)\n",
    "\n",
    "cntr = 0\n",
    "# Loop through each food subclass\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    class_path = os.path.join(SRC_DIR, class_name)\n",
    "\n",
    "    # Ensure its a folder\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # Shuffle images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Set splitting point\n",
    "        split_idx = int(len(images) * 0.8)\n",
    "        \n",
    "        train_images = images[:split_idx]\n",
    "        test_images = images[split_idx:]\n",
    "\n",
    "        # Create new food folders in train and test\n",
    "        train_class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        test_class_dir = os.path.join(TEST_DIR, class_name)\n",
    "        os.makedirs(train_class_dir)\n",
    "        os.makedirs(test_class_dir)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        cntr += 1\n",
    "        print(f\"{cntr}. Processesed {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "print(\"Dataset split successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
