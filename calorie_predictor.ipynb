{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gCQagsTmP2Y"
   },
   "source": [
    "# 1. Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qlq6CvnuS6w"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for API querying\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# Imports for image scaling and file access\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For splitting into train and test\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# For EDA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# For calculations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be loading API keys and related secret value we will be using .env files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function which will be useful later on in nutrition data retrieval will be to get a list of all the food names present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W4JKIDQwPZq",
    "outputId": "51759af3-4dd2-4d1c-bc6e-a43896271864"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_food_names():\n",
    "    data_path = 'food-128'\n",
    "    food_names = []\n",
    "\n",
    "    # Extract food names from folders\n",
    "    for food_name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, food_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            food_names.append(food_name.replace('_', ' '))\n",
    "    return food_names\n",
    "print(get_food_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the images will help massively in getting a working model without needing to use the full image data. The method to do so is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6opMW_FUzvtJ"
   },
   "outputs": [],
   "source": [
    "# Downscale images (ChatGPT)\n",
    "# Take our raw images\n",
    "\n",
    "# Source and target directories\n",
    "SRC_DIR = \"images\"        # original images folder with class subfolders\n",
    "DST_DIR = \"food-128\"      # output folder\n",
    "\n",
    "size = (128, 128)  # target resolution\n",
    "\n",
    "# Create output root directory\n",
    "os.makedirs(DST_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through every class folder\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    src_class_path = os.path.join(SRC_DIR, class_name)\n",
    "    dst_class_path = os.path.join(DST_DIR, class_name)\n",
    "\n",
    "    # Skip non-directories (just in case)\n",
    "    if not os.path.isdir(src_class_path):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(dst_class_path, exist_ok=True)\n",
    "\n",
    "    # Resize each image\n",
    "    for img_name in tqdm(os.listdir(src_class_path), desc=f\"Resizing {class_name[:20]}\"):\n",
    "        src_img_path = os.path.join(src_class_path, img_name)\n",
    "        dst_img_path = os.path.join(dst_class_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_img_path).convert(\"RGB\")\n",
    "            img = img.resize(size, Image.Resampling.BILINEAR)\n",
    "            img.save(dst_img_path, quality=90)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {src_img_path}: {e}\")\n",
    "\n",
    "print(\"All images resized to 128x128 and saved in food-128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FoodData Central API Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading the API data, we'll first need to define some constants which will be used through data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup\n",
    "FDC_BASE_URL = 'https://api.nal.usda.gov/fdc/v1'\n",
    "FDC_API_KEY = os.getenv('FDC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search the FoodData Central database, we'll use the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_food(query, data_type='Survey (FNDDS)', limit=10):\n",
    "    # data_type can be any of 'Branded', 'Foundation', 'Survey (FNDDS)', 'SR Legacy'\n",
    "    parameters = {\n",
    "        'api_key': FDC_API_KEY,\n",
    "        'query': query,\n",
    "        'dataType': data_type,\n",
    "        'pageSize': limit\n",
    "    }\n",
    "    response = requests.get(url=f'{FDC_BASE_URL}/foods/search', params=parameters)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_calories(food):\n",
    "    for nutrient in food.get('foodNutrients', []):\n",
    "        # TODO: test if {foodNutrientId': 34350077, nutrientName: 'Energy'} is the correct dict\n",
    "        if nutrient.get('nutrientName') == 'Energy' and nutrient.get('unitName') == 'KCAL':\n",
    "            return nutrient['value']\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_results(data):\n",
    "    foods = data.get('foods', [])\n",
    "    result_foods = []\n",
    "    for food in foods:\n",
    "        calories = get_calories(food)\n",
    "        if calories is not None:\n",
    "            result_foods.append({\n",
    "                'name': food['description'],\n",
    "                'calories': calories\n",
    "            })\n",
    "    return result_foods\n",
    "\n",
    "\n",
    "def estimate_calories_from_results(results, top_n=5):\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Get the first `top_n` calorie values that aren't None\n",
    "    calories = [r['calories'] for r in results if r.get('calories') is not None][:top_n]\n",
    "    if not calories:\n",
    "        return None\n",
    "    \n",
    "    return round(sum(calories) / len(calories), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the helper functions' ability to query the API, we can search with a given dish name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the API calls could fail, use a try except block\n",
    "try:\n",
    "    food = input('Type a food:')\n",
    "    response = search_food(food)\n",
    "    simplified = simplify_results(response)\n",
    "    print(len(simplified))\n",
    "    pprint(simplified)\n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to refine this data to contain only the relevant caloric information for the foods in our dataset. An important aspect of this to note is that the FoodData Central API is limited to 1000 requests per hour by IP address, and will block an IP address for one hour if this limit is exceded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    food_names = get_food_names()\n",
    "    all_calorie_data = []\n",
    "    for food in food_names:\n",
    "        response = search_food(food)\n",
    "        simplified = simplify_results(response)\n",
    "        estimate = estimate_calories_from_results(simplified, top_n=5)\n",
    "        print(f\"\\n{food}: ~{estimate} kcal/100g\")\n",
    "        all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': simplified\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous code block, we now have a list of the results of the keyword search for all 101 dishes along with an estimate for kcal/100g. However, some of these contained no entries at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and record missing dishes\n",
    "missing_dishes = []\n",
    "for i in range(len(food_names)):\n",
    "    if all_calorie_data[i].get('estimate_calories_per_100g') == None:\n",
    "        missing_dishes.append(food_names[i])\n",
    "\n",
    "print('Missing dish count:', len(missing_dishes))\n",
    "pprint(missing_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To supplement the data which doesn't have an entry in the FoodData Central API, we'll be using the Nutritionix API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRITIONIX_URL = 'https://trackapi.nutritionix.com/v2/natural/nutrients'\n",
    "NUTRITIONIX_APP_ID = os.getenv('NUTRITIONIX_APP_ID')\n",
    "NUTRITIONIX_APP_KEY = os.getenv('NUTRITIONIX_APP_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calories_nutritionix(query):\n",
    "    headers = {\n",
    "        'x-app-id': NUTRITIONIX_APP_ID,\n",
    "        'x-app-key': NUTRITIONIX_APP_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    body = {\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(NUTRITIONIX_URL, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    foods = data.get(\"foods\", [])\n",
    "    if not foods:\n",
    "        print(\"No nutrition data found for\", query)\n",
    "        return None\n",
    "\n",
    "    # Extract calorie info from first result\n",
    "    f = foods[0]\n",
    "    cal = f.get(\"nf_calories\", 0)\n",
    "    weight = f.get(\"serving_weight_grams\", 100)\n",
    "    cal_per_100g = cal * (100 / weight)\n",
    "\n",
    "    return round(cal_per_100g, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When storing this with the rest of our data which had FoodData Central simplified results included, we'll use `None` as the value for the `'results'` key in the food dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food in missing_dishes:\n",
    "    estimate = get_calories_nutritionix(food)\n",
    "    all_calorie_data.append({\n",
    "            'food': food,\n",
    "            'estimate_calories_per_100g': estimate,\n",
    "            'results': None\n",
    "        })\n",
    "    print(f'{food}: ', estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for food_entry in all_calorie_data:\n",
    "    pprint(food_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier use later on, we'll save the calorie estimate information to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe then store to CSV\n",
    "rows = [\n",
    "    {'food': entry['food'], 'kcal/100g': entry['estimate_calories_per_100g']}\n",
    "    for entry in all_calorie_data\n",
    "]\n",
    "\n",
    "calorie_df = pd.DataFrame(rows, columns=['food', 'kcal/100g'])\n",
    "print(calorie_df.info())\n",
    "\n",
    "calorie_df.to_csv('data/calories.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn Image Data Into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = 'food-128'\n",
    "\n",
    "TRAIN_DIR = 'food-128-split/train'\n",
    "TEST_DIR = 'food-128-split/test'\n",
    "\n",
    "# Output directories\n",
    "os.makedirs(TRAIN_DIR)\n",
    "os.makedirs(TEST_DIR)\n",
    "\n",
    "cntr = 0\n",
    "# Loop through each food subclass\n",
    "for class_name in os.listdir(SRC_DIR):\n",
    "    class_path = os.path.join(SRC_DIR, class_name)\n",
    "\n",
    "    # Ensure its a folder\n",
    "    if os.path.isdir(class_path):\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # Shuffle images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Set splitting point\n",
    "        split_idx = int(len(images) * 0.8)\n",
    "        \n",
    "        train_images = images[:split_idx]\n",
    "        test_images = images[split_idx:]\n",
    "\n",
    "        # Create new food folders in train and test\n",
    "        train_class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        test_class_dir = os.path.join(TEST_DIR, class_name)\n",
    "        os.makedirs(train_class_dir)\n",
    "        os.makedirs(test_class_dir)\n",
    "\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        cntr += 1\n",
    "        print(f\"{cntr}. Processesed {class_name}: {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "print(\"Dataset split successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Images to new format using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        # These numbers come from ImageNet\n",
    "        # Represent mean and standard deviation values for RGB based on a massive dateset of color distribution in natural images\n",
    "        mean = [0.485, 0.456, 0.406],   \n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load training and test sets\n",
    "train_dataset = datasets.ImageFolder(root = 'food-128-split/train', transform = transform)\n",
    "test_dataset = datasets.ImageFolder(root = 'food-128-split/test', transform = transform)\n",
    "\n",
    "# Create PyTorch data loaders for later\n",
    "train_data = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = 32, # use 32 images per batch\n",
    "    shuffle = True, # shuffle training data each time to not get false patterns\n",
    "    num_workers = 4 # use 4 CPU threads in parallel to speed up work\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 32, # same as above\n",
    "    shuffle = False, # don't shuffle test data for consistent benchmarks\n",
    "    num_workers = 4 # same as above\n",
    ")\n",
    "\n",
    "# Verify stuff\n",
    "class_cnt = len(train_dataset.classes)\n",
    "# Get class count\n",
    "print(f\"Num Classes: {class_cnt}\")\n",
    "# See a few just to verify names saved correctly\n",
    "print(f\"Sample Classes: {train_dataset.classes[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Calorie Estimate EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calorie data loading and summary statistics\n",
    "CALORIE_FILE_PATH = 'data/calories.csv'\n",
    "calorie_df = pd.read_csv(CALORIE_FILE_PATH)\n",
    "\n",
    "print(calorie_df.info())\n",
    "print(calorie_df.describe())\n",
    "print(calorie_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Calorie Visualization\n",
    "# Begin by sorting df by 'kcal/100g'\n",
    "calorie_df.sort_values('kcal/100g', ascending=False, inplace=True)\n",
    "calorie_df.reindex()\n",
    "\n",
    "# Create bar chart of the kcal/100g estimates for each food\n",
    "plt.figure(figsize=(5, 18))\n",
    "plt.title('Bar Chart of Calorie Estimates')\n",
    "plt.barh(calorie_df['food'], calorie_df['kcal/100g'])\n",
    "plt.ylabel('Food Name')\n",
    "plt.xlabel('Estimated kcal/100g')\n",
    "plt.margins(y = 0.005)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data EDA\n",
    "Generate visuals for the average pixel values of each food class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Images to tensor (fancy 4D array)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset using this transform\n",
    "dataset = datasets.ImageFolder(root='food-128-split/train', transform = transform)\n",
    "\n",
    "# Get names of all foods\n",
    "food_names = dataset.classes\n",
    "total_num_foods = len(food_names) # should always be 101 but just in case\n",
    "\n",
    "# 7 distinct food to visualize\n",
    "selected_classes = [\n",
    "    'sushi',\n",
    "    'guacamole',\n",
    "    'spaghetti_bolognese',\n",
    "    'macarons',\n",
    "    'chocolate_cake',\n",
    "    'caesar_salad',\n",
    "    'strawberry_shortcake',\n",
    "    'beef_carpaccio'\n",
    "]\n",
    "food_count = len(selected_classes)\n",
    "\n",
    "# Map class names to integer labels used by ImageFolder\n",
    "selected_indices = [dataset.class_to_idx[c] for c in selected_classes]\n",
    "\n",
    "# Filter dataset to only images with those labels\n",
    "subset_indices = [\n",
    "    i for i, (_, label) in enumerate(dataset.samples)\n",
    "    if label in selected_indices\n",
    "]\n",
    "subset_dataset = Subset(dataset, subset_indices) # create subset of just selected indices\n",
    "\n",
    "# Create data loader because its much faster\n",
    "subset_loader = DataLoader(\n",
    "    subset_dataset,\n",
    "    batch_size=32,   # process 32 images at once\n",
    "    shuffle=False,   # don't randomize order for this\n",
    "    num_workers=4    # load using 4 CPU threads in parallel\n",
    ")\n",
    "\n",
    "# Get tensors ready to find averages\n",
    "H, W = 128, 128\n",
    "# Track sum of R,G,B values for each pixel position\n",
    "# Torch.zeros is 4D array we can use to keep values for each pixel distinct\n",
    "sums = torch.zeros((food_count, 3, H, W))\n",
    "# Keep track of how many images you've seen in current food class (again should always be 101 for our case)\n",
    "counts = torch.zeros(food_count)\n",
    "# Keep track of running sum of squares\n",
    "sum_sq = torch.zeros((food_count, 3, H, W))\n",
    "\n",
    "# Map original labels (0-100) to new compact label (0..food_count-1)\n",
    "class_map = {orig_label: i for i, orig_label in enumerate(selected_indices)}\n",
    "\n",
    "# Load through batches and add up pixel values\n",
    "for images, labels in subset_loader: # images is tensor of batch size, 3, H, W --- lables is tensor of batch size with label for each image (will be the same label except when transitioning between food types)\n",
    "    for i in range(images.size(0)):  # iterate through each image in batch\n",
    "        label = labels[i].item() # get current image label\n",
    "        if label in class_map:\n",
    "            idx = class_map[label] # get idx in new smaller array\n",
    "            sums[idx] += images[i] # add current image pixel values\n",
    "            counts[idx] += 1 # track that we've added another image worth of pixels for later division\n",
    "            sum_sq[idx] += images[i] ** 2\n",
    "\n",
    "# Calculate average image (pixel-wise division)\n",
    "# view reshapes counts into food_count, 1, 1, 1 so it works correctly when dividing into 4D sums tensor\n",
    "avg_images = (sums / counts.view(-1, 1, 1, 1))\n",
    "\n",
    "# Pixel-wise standard deviation\n",
    "std_images = torch.sqrt((sum_sq / counts.view(-1, 1, 1, 1)) - (avg_images ** 2))\n",
    "\n",
    "# Get std between different classes\n",
    "interclass_std = torch.std(avg_images, dim=0)\n",
    "\n",
    "# Calculate average pixel for each average image\n",
    "avg_pixels = avg_images.mean(dim=(2, 3)) # avg_pixels is a tensor of batch size, 3, where each of the values are the means from dims H and W or avg_images\n",
    "\n",
    "# Visualize average images\n",
    "fig, axes = plt.subplots(math.ceil(food_count / 2), 2, figsize=(5, math.ceil(food_count / 2) * 2.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, orig_label in enumerate(selected_indices):\n",
    "    avg_img = avg_images[i].permute(1, 2, 0).numpy()  # Pytorch keeps images in Color, Height, Width, Order but matplotlib uses Height, Width, Color\n",
    "    axes[i].imshow(avg_img)\n",
    "    axes[i].set_title(food_names[orig_label], fontsize=12)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize average pixel channel values\n",
    "fig, axes = plt.subplots(math.ceil(food_count / 2), 2, figsize=(6, math.ceil(food_count / 2) * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, orig_label in enumerate(selected_indices):\n",
    "    channel_names = ['R', 'G', 'B']\n",
    "    values = [avg_pixels[i, 0], avg_pixels[i, 1], avg_pixels[i, 2]]\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    axes[i].bar(channel_names, values, color=colors)\n",
    "    axes[i].set_title(food_names[orig_label])\n",
    "    axes[i].set_xlabel('Color Channel')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].set_xmargin(0.1)\n",
    "    axes[i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute overall std value for each class\n",
    "class_std_values = std_images.mean(dim=(1, 2, 3))\n",
    "\n",
    "# Plot std values for deviation in average color between classes\n",
    "# Possible future improvement could be using a circlular subset of the image to get the average color to avoid background noise\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh([food_names[i] for i in selected_indices], class_std_values.numpy())\n",
    "plt.xlabel(\"Average Standard Deviation\")\n",
    "plt.title(\"Overall Color Variability per Food Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert to numpy image and reorder to HWC for matplotlib\n",
    "interclass_std_img = interclass_std.permute(1, 2, 0).numpy()\n",
    "# Normalize for better contrast (to highlight areas with most deviation)\n",
    "interclass_std_img = interclass_std_img / interclass_std_img.max()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(interclass_std_img)\n",
    "plt.title(\"Standard Deviation Across Classes\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Numerical Summaries for Entire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels and image dimensions (assume all images have same shape)\n",
    "sample_img, _ = dataset[0]\n",
    "num_channels, H, W = sample_img.shape\n",
    "\n",
    "# Count images per class\n",
    "labels = [label for _, label in dataset]\n",
    "counts_per_class = Counter(labels)\n",
    "\n",
    "# Map label index to class name\n",
    "class_names = dataset.classes\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "print(f\"Image shape: {num_channels} channels, {H}x{W} pixels\")\n",
    "print(\"Number of images per class:\")\n",
    "for label, count in counts_per_class.items():\n",
    "    print(f\"  {class_names[label]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "Since we're working with 2D image data, using a CNN for classification is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define a basic CNN using PyTorch's Sequential model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN architecture\n",
    "\n",
    "# Layer filter counts\n",
    "num_conv1 = 32\n",
    "num_conv2 = 64\n",
    "num_conv3 = 128\n",
    "num_conv4 = 256\n",
    "num_hidden5 = 512\n",
    "num_output = 101\n",
    "\n",
    "num_convs = 4\n",
    "image_shape = 128\n",
    "k_size = 3          # kernel size\n",
    "p_size = 2          # pool size\n",
    "padding = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Block 1\n",
    "    #   Using in_channels=3 because of the three color channels. Each filter has each dimension halved.\n",
    "    nn.Conv2d(in_channels=3, out_channels=num_conv1, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 2\n",
    "    nn.Conv2d(in_channels=num_conv1, out_channels=num_conv2, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 3\n",
    "    nn.Conv2d(in_channels=num_conv2, out_channels=num_conv3, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Block 4\n",
    "    nn.Conv2d(in_channels=num_conv3, out_channels=num_conv4, kernel_size=k_size, padding=padding),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=p_size),\n",
    "\n",
    "    # Classification\n",
    "    #   Hidden layer takes num_conv4 filters each of dimensions (image_shape // 2 ** num_convs, image_shape // 2 ** num_convs).\n",
    "    #   This layer produces the final output.\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=num_conv4 * (image_shape // 2 ** num_convs) ** 2, out_features=num_hidden5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_hidden5, out_features=num_output)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the CNN\n",
    "\n",
    "# Move to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Define criterion and optimizer to use in training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_data):\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# CNN Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import split_dataset\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101000 files belonging to 101 classes.\n",
      "Using 70700 files for training.\n",
      "Found 101000 files belonging to 101 classes.\n",
      "Using 30300 files for validation.\n",
      "0.0 1.0\n",
      "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Data Loading Pipeline\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "seed = 42\n",
    "\n",
    "# 70% train, 30% temp split\n",
    "train_data = image_dataset_from_directory(\n",
    "    'food-128',\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "temp_data = image_dataset_from_directory(\n",
    "    'food-128',\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width)\n",
    ")\n",
    "\n",
    "# Split temp_data into 15% val, 15% test\n",
    "temp_batches = len(temp_data)\n",
    "val_batches = temp_batches // 2\n",
    "\n",
    "val_data = temp_data.take(val_batches)\n",
    "test_data = temp_data.skip(val_batches)\n",
    "\n",
    "# Normalize\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "normalized_ds = train_data.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# pixel values now in range 0,1\n",
    "print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "\n",
    "\n",
    "class_names = train_data.class_names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 128, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,719,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,813</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m4,719,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)            │        \u001b[38;5;34m51,813\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,159,333</span> (19.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,159,333\u001b[0m (19.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,159,333</span> (19.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,159,333\u001b[0m (19.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 107/2210\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:48\u001b[0m 137ms/step - accuracy: 0.0098 - loss: 20.2031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Fit the model to the data\u001b[39;00m\n\u001b[32m     49\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Defining the CNN architecture in tensorflow\n",
    "\n",
    "# Layer filter counts\n",
    "num_conv1 = 32\n",
    "num_conv2 = 64\n",
    "num_conv3 = 128\n",
    "num_conv4 = 256\n",
    "num_hidden5 = 512\n",
    "num_output = 101\n",
    "\n",
    "num_convs = 4\n",
    "image_shape = 128\n",
    "k_size = 3          # kernel size\n",
    "p_size = 2          # pool size\n",
    "padding = 1\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    # Block 1\n",
    "    Conv2D(num_conv1, kernel_size=(k_size, k_size), activation='relu', input_shape=(image_shape, image_shape, 3)),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "    \n",
    "    # Block 2\n",
    "    Conv2D(num_conv2, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Block 3\n",
    "    Conv2D(num_conv3, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Block 4\n",
    "    Conv2D(num_conv4, kernel_size=(k_size, k_size), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(p_size, p_size)),\n",
    "\n",
    "    # Classification Head\n",
    "    #   Hidden layer takes num_conv4 filters each of dimensions (image_shape // 2 ** num_convs, image_shape // 2 ** num_convs).\n",
    "    #   This layer produces the final output.\n",
    "    Flatten(),\n",
    "    Dense(num_hidden5, activation='relu'),\n",
    "    Dense(num_output, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "num_epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x000001C8CB73BE00> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m train_ds = train_data.map(preprocess, num_parallel_calls=AUTOTUNE)\n\u001b[32m     17\u001b[39m test_ds = test_data.map(preprocess, num_parallel_calls=AUTOTUNE)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m model = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://www.kaggle.com/models/spsayakpaul/vision-transformer/TensorFlow2/vit-b8-classification/1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m predictions = model.predict(images) \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:75\u001b[39m, in \u001b[36mSequential.__init__\u001b[39m\u001b[34m(self, layers, trainable, name)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m layers:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrebuild\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_rebuild()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paul_\\VS Code Projects\\DataScience413\\DataScienceFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:97\u001b[39m, in \u001b[36mSequential.add\u001b[39m\u001b[34m(self, layer, rebuild)\u001b[39m\n\u001b[32m     95\u001b[39m         layer = origin_layer\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOnly instances of `keras.Layer` can be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33madded to a Sequential model. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_layer_name_unique(layer):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAll layers added to a Sequential model \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshould have unique names. Name \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is already \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe name of a layer in this model. Update the `name` argument \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto pass a unique name.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x000001C8CB73BE00> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "image_size = 224\n",
    "images = test_data\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess(image, label):\n",
    "    # Only resize + normalize (no augmentation)\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    image = (tf.cast(image, tf.float32) *2) - 1.0\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_data.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_data.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://www.kaggle.com/models/spsayakpaul/vision-transformer/TensorFlow2/vit-b8-classification/1\")\n",
    "])\n",
    "predictions = model.predict(images) \n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate\n",
    "# ---------------------------\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m474/474\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 55ms/step - accuracy: 0.0100 - loss: 4.6147\n",
      "Test Loss: 4.6147, Test Accuracy: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
